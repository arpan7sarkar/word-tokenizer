        class SubwordTokenizer {
            constructor() {
                this.vocab = new Map();
                this.idToToken = new Map();
                this.nextId = 0;
                this.addToken('<PAD>'); // Padding token
            }

            addToken(token) {
                if (!this.vocab.has(token)) {
                    this.vocab.set(token, this.nextId);
                    this.idToToken.set(this.nextId, token);
                    this.nextId++;
                }
            }
            
            // Fit on text by adding all characters, spaces, and words to the vocabulary
            fitOnText(text) {
                // Add all individual characters to ensure we can tokenize anything
                for (const char of text) {
                    this.addToken(char.toLowerCase());
                }
                // Add whole words and spaces to the vocabulary
                const parts = text.toLowerCase().split(/(\s+)/);
                for (const part of parts) {
                    if (part) { // Ensure not adding empty strings
                        this.addToken(part);
                    }
                }
            }

            // The core subword tokenization logic
            tokenize(text) {
                // Split text by spaces, but keep the spaces as tokens
                const parts = text.toLowerCase().split(/(\s+)/);
                let subwordTokens = [];

                for (const part of parts) {
                    if (!part) continue; // Skip empty strings from split

                    // If the part is just whitespace, it's its own token
                    if (/^\s+$/.test(part)) {
                        subwordTokens.push(part);
                        continue;
                    }

                    // Otherwise, it's a word; break it down using the greedy algorithm
                    let start = 0;
                    while (start < part.length) {
                        let bestMatch = '';
                        // Find the longest subword in our vocab that matches
                        for (let end = part.length; end > start; end--) {
                            const sub = part.substring(start, end);
                            if (this.vocab.has(sub)) {
                                bestMatch = sub;
                                break;
                            }
                        }
                        
                        // Fallback to a single character if no subword is found
                        if (bestMatch === '') {
                            bestMatch = part[start];
                        }

                        subwordTokens.push(bestMatch);
                        start += bestMatch.length;
                    }
                }
                return subwordTokens;
            }

            encode(tokens) {
                // Assumes tokens are already generated by the `tokenize` method
                return tokens.map(token => this.vocab.get(token));
            }

            decode(ids) {
                // Joins all tokens (including spaces) back together
                return ids.map(id => this.idToToken.get(id) || '').join('');
            }
        }

        // --- Main Application Logic ---
        document.addEventListener('DOMContentLoaded', () => {
            const encoderInput = document.getElementById('encoder-input');
            const subwordTokensOutput = document.getElementById('subword-tokens-output');
            const numberTokensOutput = document.getElementById('number-tokens-output');
            const vocabularyOutput = document.getElementById('vocabulary-output');
            const decoderInput = document.getElementById('decoder-input');
            const decodedTextOutput = document.getElementById('decoded-text-output');
            const presetSelect = document.getElementById('preset-select');
            const loadPresetBtn = document.getElementById('load-preset-btn');
            const resetBtn = document.getElementById('reset-btn');

            let tokenizer = new SubwordTokenizer();

            const presets = {
                nursery: "Twinkle, twinkle, little star, How I wonder what you are.",
                quote: "The only thing we have to fear is fear itself.",
                tongue_twister: "She sells seashells by the seashore."
            };

            const updateVocabularyDisplay = () => {
                vocabularyOutput.innerHTML = '';
                const sortedVocab = [...tokenizer.vocab.entries()].sort((a, b) => a[1] - b[1]);
                for (const [token, id] of sortedVocab) {
                    const item = document.createElement('div');
                    item.className = 'vocab-item';
                    item.innerHTML = `<span class="font-mono text-gray-700">'${token}'</span> <span class="font-bold text-gray-500">${id}</span>`;
                    vocabularyOutput.appendChild(item);
                }
            };

            const renderTokens = (tokens, targetElement, cssClass) => {
                targetElement.innerHTML = '';
                tokens.forEach(token => {
                    const span = document.createElement('span');
                    span.className = `token ${cssClass}`;
                    span.textContent = token;
                    targetElement.appendChild(span);
                });
            };
            
            const handleEncoding = (text) => {
                tokenizer.fitOnText(text);
                const subwordTokens = tokenizer.tokenize(text);
                const numberTokens = tokenizer.encode(subwordTokens);
                renderTokens(subwordTokens, subwordTokensOutput, 'subword-token');
                renderTokens(numberTokens, numberTokensOutput, 'number-token');
                updateVocabularyDisplay();
            };

            const resetAll = () => {
                tokenizer = new SubwordTokenizer();
                encoderInput.value = '';
                decoderInput.value = '';
                subwordTokensOutput.innerHTML = '';
                numberTokensOutput.innerHTML = '';
                decodedTextOutput.innerHTML = '';
                updateVocabularyDisplay();
            };

            // --- Event Listeners ---
            encoderInput.addEventListener('input', (e) => {
                handleEncoding(e.target.value);
            });

            decoderInput.addEventListener('input', (e) => {
                const text = e.target.value;
                const ids = text.split(',').map(s => s.trim()).filter(s => s !== '').map(s => parseInt(s, 10)).filter(n => !isNaN(n));
                decodedTextOutput.textContent = tokenizer.decode(ids);
            });
            
            loadPresetBtn.addEventListener('click', () => {
                resetAll();
                const selectedPreset = presetSelect.value;
                const presetText = presets[selectedPreset];
                encoderInput.value = presetText;
                handleEncoding(presetText);
            });

            resetBtn.addEventListener('click', resetAll);

            // Initialize with a base vocabulary
            handleEncoding("");
            encoderInput.value = '';
            subwordTokensOutput.innerHTML = '';
            numberTokensOutput.innerHTML = '';

        });
